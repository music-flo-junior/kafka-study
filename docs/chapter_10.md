### 챕터 10. 클러스터간 데이터 미러링 하기

이 책에서는 주로 단일 카프카 클러스터를 설치하고, 운영하고, 사용하는 방법에 대해서 설명하였다. (실무에서도 단일 카프카 클러스터를 사용하고 있음)

하지만 하나 이상의 클러스터로 구성되는 아키텍처가 필요한 경우가 있다.

> 2개 이상의 클러스터가 서로 완전히 분리되어 있는 경우
- 각 클러스터가 다른 용도로 사용되기 때문에 한쪽에 있는 데이터를 다른 쪽으로 복사할 이유가 없는 경우이다.
- 이러한 사례는 꽤 쉬운데, 여러 개의 별도 클러스터를 운영하는 것은 단일 클러스터를 여러 개 운영하는 것과 같은 것이다.

> 운영자가 상호 의존하는 클러스터 사이에 데이터를 지속적으로 복사해 줘야 하는 경우
- 클러스터에 속한 카프카 노드 간의 데이터의 교환을 복제라고 부르고 있으므로, 카프카 클러스터 간의 데이터 복제는 '미러링' 이라고 부를 것이다.
- 아파치 카프카에는 클러스터간 데이터 복제를 수행하기 위한 툴로 '미러메이커' 를 포함하고 있다.

### 10.1 클러스터간 미러링 활용 사례

다음 사례들은 클러스터간 미러링이 활용될 수 있는 사례들이다.

#### 지역 및 중앙 클러스터 

- 하나의 기업이 지리적으로 분산된 지역, 도시, 대륙 간에 하나 이상의 데이터센터를 가지고 있을 수 있으며, 각각의 데이터센터에 카프카 클러스터가 설치되어 있는 경우다.
- 예를 들어 한 회사는 사무실을 운영중인 각각의 도시에 데이터 센터를 가지고 각 지역의 수요와 공급에 대한 정보를 수집해서 그에 따라 가격을 조정한다.
- 이 모든 데이터는 그 후 중앙 클러스터로 미러링되어 비즈니스 분석가들이 회사 단위의 수익 보고를 할 때 사용할 수 있다.

#### 고가용성 & 재해 복구

- 전체 클러스터가 어떠한 이유에서든 사용 불가능하게 될 가능성은 우려스럽다. 
- 첫번째 클러스터의 모든 데이터를 보유하는 여분의 두 번째 클러스터를 준비해 뒀다가 만약의 사태가 발생했을 때, 
- 애플리케이션을 두 번째 클러스터를 사용해서 작동하도록 함으로써 평상시처럼 작업을 계속하게 할 수 있다.

#### 규제

- 국가별로 다른 법적, 규제적 요구 조건을 따르기 위해 나라마다 있는 카프카 클러스터별로 서로 다른 설정과 정책을 시행해야 할 수 있다.
- 예를 들어서, 어떠한 데이터는 엄격한 접근 설정과 함께 서로 분리된 클러스터에 저장한 뒤 그 중 일부는 보다 개방적인 접근 설정이 되어 있는 클러스터로 복제하는 식이다.

#### 클라우드 마이그레이션

- 요즘은 각각의 온프레미스 데이터센터와 각각의 클라우드 리전에 최소 1개의 카프카 클러스터가 존재하는 것이 보통이다.
- 이러한 카프카 클러스터들은 데이터센터 간에 데이터를 효율적으로 전송하기 위해 각각의 데이터센터나 리전에 위치한 어플리케이션에 의해 사용된다.
- 예를 들어서, 클라우드 환경에서 온프레미스 데이터베이스에 저장되는 데이터를 필요로 한다거나 할 경우, 카프카 커넥트를 사용해서 데이터베이스의 변경 내역을 로컬 카프카 클러스터로 내보낸 뒤 이를 다시 클라우드 환경에서 실행되는 카프카 클러스터로 미러링함으로써 클라우드 환경에서도 이 데이터를 사용할 수 있도록 할 수 있다.
- 이러한 방식은 데이터 센터 간의 트래픽을 관리하고 보안을 유지하는 데 도움을 줄 뿐만 아니라 비용 역시 절감할 수 있게 해 준다.

#### 엣지 클러스터로부터의 데이터 집적 ( : 모아 쌓는 것 )

- 유통, 통신, 물류에서 헬스케어에 이르는 여러 산업에서는 연결성이 제한된 소형 기기를 사용해서 데이터를 생성한다.
- 가용성이 높은 집적용 클러스터를 사용한다면 많은 수의 엣지 클러스터로부터 수집된 데이터를 분석하는 등의 용도로 사용될 수 있다.
- 엣지 클러스터 란?
- 네트워크의 클라이언트 또는 디바이스에게 컴퓨팅 파워를 분산해주는 기술 구조
- 중앙 집중형 Cloud는 대용량/고성능 처리가 필요한 워크로드에 집중하고, Edge 에서는 초저지연/초연결이 필요한 워크로드를 수행하면서 상호 보완적 모델을 구현한다.
- 참고 : https://tech.ktcloud.com/48

- 가용성이 높은 집적용 클러스터는 엣지 클러스터가 오프라인 상태가 되거나 할 경우에도 연속적으로 작업을 수행할 수 있게 해줄 뿐 아니라 불안정한 네트워크로 연결되어 있는 많은 수의 엣지 클러스터를 직접적으로 신경 쓸 필요가 없도록 한다.

### 10.2 다중 클러스터 아키텍처

위 활용 사례를 구현할 때 성공적으로 사용되어 온 공통적인 아키텍처 패턴들을 살펴보자.

아키텍처를 살펴보기 전에 데이터센터 간 통신에서 현실적으로 고려해야 할 사항들에 대해서 간략히 살펴보자.

#### 10.2.1 데이터센터간 통신의 현실적 문제들

#### 높은 지연

- 두 카프카 클러스터 간의 거리나 네트워크 홉 개수가 증가함에 따라 통신 지연 역시 증가한다.

#### 제한된 대역폭

- 광역 통신망은 일반적으로 단일 데이터센터 내부보다 훨씬 낮은 대역폭을 가지며, 사용 가능한 대역폭 역시 시시각각 변하는 특성을 가지고 있다.
- 뿐만 아니라 지연이 높아지는 만큼 사용 가능한 대역폭을 최대한 활용하는 것 역시 더 어려워진다.

#### 더 높은 비용

- 카프카를 온프레미스에서 운영하든 클라우드에서 운영하든 간에, 클러스터 간의 통신에는 더 많은 비용이 든다.
- 공급자가 서로 다른 데이터센터, 리전, 클라우드와의 데이터 전송에 과금하기 때문이다.


카프카 클러스터 아키텍처를 구성할 떄, 아래의 원칙을 참고하자

1. 하나의 데이터센터 당 한 개 이상의 클러스터를 설치한다.
- 카프카의 브로커와 클라이언트는 하나의 데이터센터 안에서 실행되도록 설계, 개발, 테스트되어 있다.
- 개발자들은 브로커와 클라이언트 사이에 낮은 지연, 높은 대역폭을 가진 상황을 상정하며, 타임아웃 기본값이나 버퍼 크기 또한 이에 맞춰 설정되어 있다.
- 따라서 카프카 브로커를 다른 데이터센터에 나눠서 설치하는 것은 권장되지 않는다.

2. 각각의 데이터센터 간에 각각의 이벤트를 (에러로 인한 재시도를 제외하면) 정확히 한 번씩 복제한다.

3. 가능하다면, 원격 데이터센터에 쓰는 것보다 원격 데이터센터에서 읽어오는게 낫다.
- 원격 데이터센터에 쓰는 작업을 하는 경우, 더 높은 통신 지연과 네트워크 에러 발생 가능성을 감수해야 하기 때문이다.

#### 10.2.2 허브-앤-스포크 아키텍처

![image](https://github.com/user-attachments/assets/cd9ac17c-f42c-493d-b931-18f58d943c83)

- 여러 개의 로컬 카프카 클러스터와 한 개의 중앙 카프카 클러스터가 있는 상황을 상정한 것이다.
- 이 아키텍처는 데이터가 여러 개의 데이터 센터에서 생성되는 반면, 일부 컨슈머는 전체 데이터를 사용해야 할 경우 사용된다.
- 각각의 데이터센터에서 실행되는 애플리케이션이 해당 데이터센터의 로컬 데이터만을 사용할 수 있게 해줄 뿐 모든 데이터센터에서 생성된 전체 데이터세트를 사용할 수 있도록 해주지는 않는다.
- 항상 로컬 데이터센터에서 데이터가 생성되고, 각각의 데이터센터에 저장된 이벤트가 중앙 데이터센터로 단 한번만 미러링된다.

#### 10.2.3 액티브-액티브 아키텍처

![image](https://github.com/user-attachments/assets/98b8e0d5-7b2e-49bd-921e-a5b959c14fb8)
- 2개 이상의 데이터센터가 전체 데이터의 일부 혹은 전체를 공유하면서, 각 데이터센터가 모두 읽기와 쓰기를 수행할 수 있어야 할 경우 사용된다.
- 허브-앤-스포크 아키텍처와 달리 사용할 수 있는 데이터가 제한됨으로써 발생하는 기능 제한이 없으며 성능상으로도 이점이 있다.
또 다른 장점은 데이터 중복, 회복 탄력성 이다.


- 주된 단점은 데이터를 여러 위치에서 비동기적으로 읽거나 변경할 경우 발생하는 충돌을 피하는 것이 어렵다는 점이다.
- 예를 들어 인터넷 서점이라면 하나의 데이터 센터에서 위시리스트에 책을 추가하고, 곧바로 다른 데이터센터에서 데이터를 읽어오는 경우 위시리스트에 방금 추가한 책이 없을 수도 있다. 이런 경우 각 사용자를 특정 데이터센터에 고정하는 방법 등을 고려해야한다.
- 같은 데이터가 클러스터 사이를 오가면서 끝없이 순환 미러링되는 것을 막아야 한다. 이것은 각각의 '논리적 토픽'에 대해 데이터센터별로 별도의 토픽을 두고, 원격 데이터센터에서 생성된 토픽의 복제를 막음으로써 가능하다.
- 예를 들어 users 토픽은 샌프란시스코에서는 SF.users, 뉴욕에서는 NYC.users라는 토픽을 두고 운영하며, NYC 데이터센터의 토픽을 샌프란시스코에 미러링하는 경우 NYC.users 라는 토픽을 샌프란시스코에 미러링하여 구분하면 순환 미러링이 방지될 수 있다.
- 미러 메이커와 같은 미러링 툴은 이와 유사한 네이밍 컨벤션을 사용함으로써 순환 미러링을 방지한다.


- 카프카 0.11.0 에서 추가된 레코드 헤더(record header) 기능은 각각의 이벤트에 데이터가 생성된 데이터센터를 태그할 수 있도록 한다. 헤더에 심어진 정보는 이벤트가 무한히 순환 미러링되는 것을 방지하거나 다른 데이터센터에서 미러링된 이벤트를 별도로 처리하는 등의 용도로도 사용될 수 있다.

#### 10.2.4 액티브-스탠바이 아키텍처
![image](https://github.com/user-attachments/assets/cba9a399-c842-4ee9-859b-9902d340b597)
- 장애 복구용으로 여분의 클러스터를 두는 방식으로 비 실행 상태 복사본을 평소에 가지고 있다가 비상시 운영자가 실제로 이 애플리케이션을 시작시키면 두 번째 클러스터를 사용해서 작동하는 식으로 작동한다.
- 아키텍처는 간단하지만 자원의 낭비가 발생하며, 일체의 데이터 유실이나 중복 없이 카프카 클러스터를 완벽하게 복구하는 것은 불가능하다는 점이다.
- 장애 복구용 클러스터를 DR(disaster recovery) 클러스터라고 하며, DR 클러스터로 어떻게 장애 복구를 하는지가 가장 중요한 문제이다.

이제부터 장애 복구에 어떠한 것들이 필요한지 살펴보자.

#### 1. 재해 복구 계획하기
- 재해 복구를 계획할 때는 두 개의 지표를 염두에 두는 것이 중요하다.
- 복구 시간 목표 (RTO) : 모든 서비스가 장애가 발생한 뒤 다시 작동을 재개할 때까지의 최대 시간
- 복구 지점 목표 (RPO) : 장애의 결과로 인해 데이터가 유실될 수 있는 최대 시간

#### 2. 계획에 없던 장애 복구에서의 데이터 유실과 불일치
- DR 클러스터는 주 클러스터의 가장 최신 메시지를 가지고 있지 못할 것이다.
- 예상치 못한 장애가 발생해서 수천 개의 메시지를 유실했을 경우, 미러링 솔루션들이 현재로서는 트랜잭션을 지원하지 않는다는 걸 명심하라.
- 이럴 경우를 대비해서 애플리케이션 단에서 유실된 메시지를 처리하는 방법을 마련해두어야한다.

#### 3. 장애 복구 이후 애플리케이션의 시작 오프셋
- 장애 복구를 할 때, 다른 클러스터로 옮겨간 애플리케이션이 데이터를 읽어오기 시작해야 하는 위치를 결정해야한다.
- 데이터 유실이나 중복을 초래할 수 있으며, 다양한 방식이 있다.


#### 자동 오프셋 재설정 
- earliest (파티션의 맨 앞에서 읽기 시작) / latest (맨 끝에서 읽기 시작)
- 중복 처리와 데이터 유실이 별 문제가 되지 않는다면 latest 가 default 로 가장 많이 사용되는 방법이다.


#### 오프셋 토픽 복제 
- __consumer_offsets 토픽에 가장 최근의 오프셋을 저장 및 이를 DR 클러스터로 미러링해주는 방식.
- 주 클러스터와 DR 클러스터간 오프셋 불일치, 오프셋 랙 등으로 인한 어느 정도의 중복 처리는 감수해야할 수 있다.


#### 시간 기반 장애 복구 
- 카프카 0.10.0 버전부터 메시지는 카프카로 전송된 시각을 가리키는 타임 스탬프 값을 갖는다.
- 0.10.1.0 부터 브로커는 타임스탬프를 기준으로 오프셋을 검색할 수 있는 인덱스와 API를 포함한다.
- 위 API를 사용하여 장애 발생 시각에 해당하는 오프셋을 구하여 이동한 뒤, 그 지점부터 읽기 작업을 시작하면 된다.
- 0.11.0 부터 추가된 타임스탬프 기준 초기화를 포함하는 다양한 오프셋 초기화를 지원하는 kafka-consumer-groups 라는 툴을 제공한다.
- kafka-consumer-groups.sh 기능을 사용하면 특정 컨슈머 그룹의 모든 토픽에 대한 컨슈머 오프셋을 특정 시점으로 초기화할 수 있게 해준다.

#### 오프셋 변환
- 미러링되는 클러스터간의 오프셋 매핑을 저장하기 위한 툴로 예전에는 아파치 카산드라와 같은 외부 데이터 저장소를 사용했다. 
- 요즘에는 미러메이커를 포함한 미러링 솔루션들이 오프셋 변환 메타 데이터를 저장하기 위해 카프카 토픽을 사용한다.
- 오프셋 매핑은 양쪽의 오프셋 값 차이가 달라질 때마다 저장된다.
- 예를 들어 주클러스터 495 오프셋이 DR 클러스터 500 오프셋에 매칭된다면 (495, 500)을 저장하는 식이다. 
- 이를 기반으로 실제 장애 복구 작업 시 오프셋 차이가 유지된다고 보고 주 클러스터의 오프셋이 550이 DR 클러스터에서는 오프셋 555으로 매핑하는 것이다.

#### 4. 장애 복구가 끝난 후
- DR 클러스터가 주 클러스터가 되고, 기존 주 클러스터가 DR 클러스터로 역할을 변경해야 한다.
- 이렇게 되면 중복, 유실이 발생하거나 기존 주 클러스터에 잔여 이벤트들이 존재할 가능성도 있다.
- 따라서, 기존 주 클러스터에 저장된 데이터와 커밋된 오프셋을 완전히 삭제한 뒤 새로운 주 클러스터에서 완전히 새것이 된 새 DR 클러스터로 미러링을 시작한다.

#### 5. 클러스터 디스커버리 관련
- 스탠바이 클러스터를 준비할 때 고려해야 할 것 중에서 가장 중요한 것 중 하나는 장애가 발생한 상황에서 애플리케이션이 장애 복구용 클러스터와 통신을 시작하는 방법을 알 수 있게 하는 것이다.
- 프로듀서나 컨슈머 설정에 주 클러스터 브로커들의 호스트 이름을 하드코딩해 넣었다면 위 작업이 어려운 일이 되므로, DNS를 써서 주 클러스터 브로커로 연결한다.
- 그리고 비상 상황이 닥치면, DNS 이름을 스탠바이 클러스터로 돌린다.

#### 10.2.5 스트레치 클러스터
- 하나의 카프카 클러스터를 여러 개의 데이터센터에 걸치도록 설계하는 방식이다.
- 스트레치 클러스터는 다중 클러스터가 아니고, 단지 하나의 클러스터일 뿐이다. 결과적으로, 두 클러스터를 동기화 시켜주는 미러링 프로세스가 필요 없다.
- 카프카의 복제 매커니즘이 평소대로 클러스터 안의 브로커들을 동기화된 상태로 유지시켜 주는 것이다. (= 동기적인 복제)


- 스트레치 클러스터는 메시지가 두 데이터센터에 위치한 카프카 브로커 각각에게 성공적으로 쓰여진 뒤에야 응답이 가도록 설정할 수 있다.
- 각 파티션이 하나 이상의 데이터센터에 분산해서 저장되도록 랙(rack) 설정을 해주고, min.insync.replicas 설정을 잡아주고, acks를 all로 잡아준다.


- 이 아키텍처는 카프카를 높은 대역폭과 낮은 지연을 가진 회선으로 서로 연결된 최소 3개의 데이터센터에 설치할 수 있을 경우에 적당하다.
- 3개의 데이터센터라는 조건이 중요한 이유는 주키퍼 클러스터 떄문이다. 
- 주키퍼 클러스터는 quorum을 유지하는 동안 서비스를 유지할 수 있으며, 과반수 이상의 서버가 다운됐을 경우 서비스는 중단되기 때문이다.

### 10.3 아파치 카프카의 미러메이커

![image](https://github.com/user-attachments/assets/3f8849cc-1969-4d3b-8e61-0ced4206b02d)

- 아파치 카프카는 두 데이터센터 간의 데이터 미러링을 위해 미러메이커라 불리는 툴을 포함한다.
- 미러메이커는 데이터베이스가 아닌 다른 카프카 클러스터로부터 데이터를 읽어오기 위해 소스 커넥터를 사용한다.
- 미러메이커에서는 각각의 태스크가 한 쌍의 컨슈머와 프로듀서로 이루어진다. 
- 미러메이커는 원본 클러스터의 각 파티션에 저장된 이벤트를 대상 클러스터의 동일한 파티션으로 미러링함으로써 파티션의 의미 구조나 각 파티션 안에서의 이벤트 순서를 그대로 유지한다.

#### 10.3.1 미러메이커 설정하기

- 미러메이커는 매우 세밀한 곳까지 설정이 가능한 시스템이다.
- 토폴로지, 카프카 커넥트, 커넥터 설정을 정의하기 위한 클러스터 설정뿐만 아니라 미러메이커가 내부적으로 사용하는 프로듀서, 컨슈머, 어드민 클라이언트의 모든 설정 매개변수도 커스터마이즈가 가능하다.


- 아래 명령을 통해 properties 파일에 정의된 설정값을 사용해서 미러메이커를 실행시킨다.

```
bin/connect-mirror-maker.sh etc/kafka/connect-mirror-maker.properties
```

> 복제흐름

- 다음 예제는 각각 뉴욕과 런던에 있는 두 개의 데이터센터 사이에 액티브-스탠바이 복제 흐름을 정의하는 설정 옵션을 보여준다.

```
clusters = NYC, LON                                     // 복제 흐름에서 사용될 클러스터의 별칭을 정의한다.
NYC.bootstrap.servers = kafka.nyc.example.com:9092      // 각 클러스터에 대한 부트스트랩 서버를 지정해준다.
LON.bootstrap.servers = kafka.lon.example.com:9092
NYC->LON.enable = true                                  // {원본}->{대상} 접두어를 사용해서 클러스터 간에 복제 흐름을 활성화 시킨다.
                                                        // 이 플로에 적용되는 모든 설정 옵션은 동일한 접두어를 사용한다.
NYC->LON.topics = .*                                    // 복제 흐름에서 미러링되는 토픽들을 정의한다.
```

> 미러링 토픽
- 각 복제 흐름에서 미러링될 토픽들을 지정하기 위해 정규식을 사용할 수 있다. -> prod.* / test.*
- 복제된 대상 토픽 이름 앞에는 기본적으로 원본 클러스터의 별칭이 접두어로 붙는다. 
- 예를 들어서, 액티브-액티브 아키텍처에서 미러메이커가 NYC 데이터센터에 있는 토픽을 LON 데이터센터로 미러링할 경우 NYC의 orders 토픽은 LON에서는 NYC.orders 토픽으로 복제된다.
- 이러한 기본 네이밍 전략은 액티브-액티브 모드에서 두 클러스터 간에 메시지가 무한히 순환 복제 되는 사태를 방지한다.
- 미러메이커는 원본 클러스터에 새로운 토픽이 추가되었는지 주기적으로 확인하고, 설정된 패턴과 일치할 경우 자동으로 이 토픽들에 대한 미러링 작업을 시작한다.
- 원본 토픽에 파티션이 추가될 경우, 대상 토픽에도 자동으로 추가되기 때문에 원본 토픽의 이벤트가 대상 토픽에서도 같은 파티션 같은 순서로 저장되게 된다.

> 컨슈머 오프셋 마이그레이션
- 미러메이커는 주 클러스터에서 DR 클러스터로 장애 복구를 수행할 때 주 클러스터에서 마지막으로 체크포인트된 오프셋을 DR 클러스터에서 찾을 수 있도록 RemoteClusterUtils 유틸리티 클래스를 포함한다.
- 이 기능은 주기적으로 주 클러스터에 커밋된 오프셋을 자동으로 변환하여 DR 클러스터의 __consumer_offsets에 커밋해 줌으로써 DR 클러스터로 옮겨가는 컨슈머들이 별도의 마이그레이션 작업 없이도 주 클러스터의 커밋 지점에 해당하는 오프셋에서 바로 작업을 재개할 수 있게 해준다.
- 단, 불의의 사고를 방지하기 위해 현재 대상 클러스터 쪽 컨슈머 그룹을 사용중인 컨슈머들이 있을 경우, 미러메이커는 오프셋을 덮어쓰지 않는다 
- 따라서 작동 중인 컨슈머들의 컨슈머 그룹 오프셋과 원본에서 마이그레이션된 오프셋이 충돌하는 사태는 발생하지 않는다.

> 토픽 설정 및 ACL 마이그레이션
- 데이터 레코드를 미러링하는 것 외에도 토픽 설정과 ACL(Access Control List)을 미러링하도록 설정할 수 있다.
- 어떤 설정이나 토픽을 포함하거나 제외할 지 커스터마이징이 가능하다.

> 커넥터 테스크
- tasks.max 설정 옵션은 미러메이커가 띄울 수 있는 커넥터 태스크의 최대 개수를 정의한다.
- 최소한 2 이상으로 잡을 것을 권장하며, 복제할 토픽 파티션이 많은 경우 병렬 처리 수준 향상을 위해 더 높게 설정한다.

> 설정 접두어
- 미러메이커에 사용되는 모든 컴포넌트들에 대해서 설정 옵션을 잡아줄 수 있다.
- 카프카 커넥트와 커넥터 설정은 별도의 접두어를 필요로 하지 않는다. 
- 하지만 미러메이커에는 2개 이상의 클러스터에 대한 설정이 포함될 수 있기 때문에 특정 클러스터나 복제 흐름에 적용되는 설정의 경우 접두어를 사용할 수 있다.
```
- {클러스터}.{커넥터 설정}
- {클러스터}.admin.{어드민 클라이언트 설정}
- {원본 클러스터}.consumer.{컨슈머 설정}
- {대상 클러스터}.producer.{프로듀서 설정}
- {원본 클러스터}->{대상 클러스터}.{복제 흐름 설정}
```

#### 10.3.2 다중 클러스터 토폴로지
- 뉴욕과 런던 사이의 액티브-액티브 토폴로지는 단순히 양방향 복제 흐름을 활성화하는 것만으로 설정이 가능하다.
- 이 경우, NYC의 모든 토픽이 LON으로 미러링되고 그 반대 역시 마찬가지지만, 미러메이커는 원격 토픽 앞에 클러스터 별칭을 붙임으로써 동일한 이벤트가 순환 복제되는 것을 방지한다.

```
clusters = NYC, LON                                    
NYC.bootstrap.servers = kafka.nyc.example.com:9092     
LON.bootstrap.servers = kafka.lon.example.com:9092
NYC->LON.enable = true                                  
NYC->LON.topics = .*                                    
LON->NYC.enable = true
LON->NYC.topics = .*
```

#### 10.3.3 미러메이커 보안

- 프로덕션 환경 클러스터의 경우, 모든 데이터 센터간 트래픽에 보안을 적용하는 것이 중요하다.
- 미러메이커의 경우 원본과 대상 클러스터 양쪽에 보안이 적용된 브로커 리스너를 사용하도록 설정 되어야 한다. 
- 또한 각 클러스터에 대해 클라이언트 쪽 보안 옵션들 역시 적용되어야 한다. 
- 모든 트래픽에는 SSL 암호화가 적용되어야 한다.

#### 10.3.4 프로덕션 환경에 미러메이커 배포하기

- 전용 모드 프로세스를 여러 개 띄움으로써 확장성과 내고장성을 갖춘 미러메이커 클러스터를 구성할 수 있다.
동일한 대상 클러스터를 갖는 프로세스들은 자동으로 서로를 인식해서 부하 균형을 맞출 것이다.


- 미러메이커는 도커 컨테이너 안에서 실행할 수 있다. 미러메이커는 상태라는 게 없기 때문에 디스크 공간을 아예 필요로 하지 않는다. (모든 필요한 데이터와 상태는 카프카에 저장된다.)


- 미러메이커가 카프카 커넥트에 기반한 만큼, 카프카 커넥트에서 미러메이커를 실행시킬 수 있다. 개발이나 테스트 용도라면 독립 실행 모드로 작동시킬 수 있다.
- 미러메이커는 별도로 설치된 분산 모드 카프카 커넥트 클러스터에서 커넥터 형태로도 실행될 수 있다. 프로덕션 환경에서 사용할 거라면, 우리는 미러메이커를 분산 모드로 실행시킬 것을 권장한다.


- 미러메이커는 가능한 한 대상 데이터센터에서 실행시키는 것이 좋다. 즉 컨슈머 쪽에 설치하는 것인데, 이는 네트워크 단절이 발생하여 데이터 센터 간 연결이 끊어졌을 경우, 이벤트를 읽어오지 못하는 것은 괜찮지만 프로듀서 자체가 동작하지 않아서 이벤트가 유실되는 게 더 크리티컬 하기 때문이다.

미러메이커를 프로덕션 환경에 배포할 때는 아래와 같이 모니터링 해야 한다.

> 카프카 커넥트 모니터링
- 커넥터 상태를 모니터링하기 위한 커넥터 지표
- 처리량을 모니터링하기 위한 소스 커넥터 지표
- 리밸런스 지연을 모니터링하기 위한 워커 지표

> 미러메이커 지표 모니터링
- 카프카 커넥트 단위에서 제공되는 지표 외에 미러링 처리량과 복제 지연을 모니터링할 수 있는 지표 제공
- replication-latency-ms: 레코드의 타임스탬프와 대상 클러스터에 성공적으로 쓰여진 시각 사이의 차이를 보여줌
- record-age-ms: 복제 시점에 레코드가 얼마나 오래되었는지
- byte-rate: 복제 처리량
- checkpoint-latency-ms: 오프셋의 마이그레이션 지연값

> 랙 모니터링
- '랙lag'이란? 원본 카프카 클러스터의 마지막 메시지 오프셋과 대상 카프카 클러스터의 마지막 메시지 오프셋 사이의 차이

> 프로듀서/컨슈머 지표 모니터링

- 미러메이커가 사용하는 카프카 커넥트 프레임워크는 프로듀서와 컨슈머를 포함한다.
- 둘 다 여러 지표들을 모니터링 가능한 형태로 제공하고 있으므로 이들을 수집하고 추척할 것을 권장한다.


    - 컨슈머: fetch-size-avg, fetch-size-max, fetch-rate, fetch-throttle-time-avg, fetch-throttle-time-max

    - 프로듀서: batch-size-avg, batch-size-max, requests-in-flight, record-retry-rate

    - 둘 다: io-ratio, io-wait-ratio

> 카나리아 테스트
- 1분에 한 번, 소스 클러스터의 특정한 토픽에 이벤트를 하나 보낸 뒤 대상 클러스터의 토픽에서 해당 메시지를 읽는 식으로 구현하여 모니터링 한다.

#### 10.3.5 미러메이커 튜닝하기
> 미러메이커 자체에 해줄 수 있는 튜닝
- 미러메이커 클러스터의 크기는 필요한 처리량과 허용할 수 있는 랙의 크기에 의해 결정된다. 만약 랙을 전혀 허용할 수 없다면 최고 수준의 처리량을 유지할 수 있는 수준으로 미러메이커의 크기를 키워줘야 한다.
- 민감한 토픽들은 별도의 미러메이커 클러스터로 분리한다.
> TCP 스택 튜닝
- TCP 스택을 튜닝해주는 것이 실질 대역폭을 증가시키는 데 도움이 될 수 있다.
- send.buffer.bytes.receive.buffer.bytes를 설정해서 프로듀서, 컨슈머의 버퍼 크기를 잡아주거
- socket.send.buffer.bytes, socket.receive.buffer.bytes 설정을 잡아주어서 브로커쪽 버퍼 크기를 설정해주는 것이 유효할 수 있다. 다만 이 때는 아래 리눅스 네트워크 설정까지 최적화가 필요하다.


- TCP 버퍼 크기 증가(net.core.rmem_default, net.core.rmem_max, net.core.wmem_default, net.core.wmem_max, net.core.optmem_max
- 자동 윈도우 스케일링 설정(cysctl -w net.ipv4.tcp_window_scaling=1 혹은 /etc/sysctl.conf 파일에 net.ipv4.tcp_window_scaling=1을 추가해 준다)
- TCP 슬로우 스타트 시간 줄이기(/proc/sys/net/ipv4/tcp_slow_start_after_idle 설정값을 0으로 잡아준다)

> 프로듀서 튜닝

- linger.ms, batch.size: batch가 충분하게 채워지지 않은 채 메시지가 발송되는데 메모리에 여유가 있는 경우 설정

- max.in.flight.requests.per.connection: 메시지 순서가 중요하다면 이 값을 1로 설정해서 응답이 오고 나서 다음 메시지가 가도록 해야한다. 만약 순서가 중요하지 않은 경우라면 5 정도로 높여주는게 처리량을 확실히 증가시켜줄 수 있는 방법일 수 있다

- fetch.max.bytes: fetch-size-avg, fetch-size-max의 크기가 이 값과 비슷하게 나오고 있는지 체크해야한다. 메모리가 충분하다면 컨슈머가 요청마다 더 많은 데이터를 가져오도록 이 값을 늘려주면 된다.

- fetch.min.bytes, fetch.max.wait.ms: fetch-rate 지표가 높은 경우 요청대비 받은 데이터가 적다는 의미이다. 두 설정값을 늘려주면 컨슈머가 한 번에 더 많은 데이터를 받게 된다.

#### 10.4 기타 클러스터간 미러링 솔루션

10.4.1 우버 uReplicator

10.4.2 링크드인 브루클린

10.4.3 컨플루언트 레플리케이터


(이런게 있구나~~~)
