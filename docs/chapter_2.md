### 챕터 2. 카프카 설치하기

### 2.1 환경 설정
###### 카프카 브로커를 설치하는 방법을 알아보자!

#### 2.1.1 운영체제 선택하기

- 아파치 카프카는 다양한 운영체제에서 실행이 가능한 자바 애플리케이션이다.
- 카프카는 윈도우, macOS, 리눅스 등 다양한 운영체제에서 실행이 가능하지만, 대체로 리눅스가 권장된다.

#### 2.1.2 자바 설치하기

- 주키퍼나 카프카를 설치하기 전에 사용 가능한 자바 환경을 먼저 설치해야 한다.
- 카프카와 주키퍼는 모든 OpenJDK 기반 자바 구현체 위에서 원활히 작동한다.
- 카프카 최신 버전은 자바 8 과 11을 모두 지원한다.

#### 2.1.3 주키퍼 설치하기

- 아파치 카프카는 카프카 클러스터의 메타데이터와 컨슈머 클라이언트에 대한 정보를 저장하기 위해 아파치 주키퍼를 사용한다.

1. 독립 실행 서버

- 데이터 /var/lib/zookeeper에 저장하는 기본적인 설정의 주키퍼를 /usr/local/zookeeper 에 설치한다.
- 클라이언트 포트로 접속해서 srvr 명령을 실행시키면 독립 실행 모드 주키퍼가 제대로 작동하는지 확인해볼 수 있다.
- (주키퍼 설치는 간단하니... 알아서 잘 하시겠져....)

2. 주키퍼 앙상블
- 주키퍼는 고가용성을 보장하기 위해 앙상블(ensemble)이라 불리는 클러스터 단위로 작동하도록 설계되었다.
- 주키퍼 부하 분산 알고리즘 때문에 앙상블은 홀수 개의 서버를 가지는 것이 권장된다.
- 좀 더 설명을 하자면..
- zookeeper를 구성하는 경우 과반수 선출(majority voting/quorums)을 위해
  zookeeper server의 수를 홀수로 구성할 것을 권고한다.

* 그렇다면  zookeeper를 짝수로 구성하면 어떠한 문제가 생기는 것인가?

- 결론적으로 말하면 그렇다고 해서 문제가 생기지는 않는다.
  다만 4대로 구성하는 경우는 결함에 대한 수준이 3대로 구성한 것과 다르지 않으며,
  6대로 구성한 경우도 5대로 구성한 경우와 다르지 않다.


- 예를들어, zookeeper server 4대로 운영하던 중 leader역할을 수행하던 zookeeper 서버에
  문제가 발생하여 3대가 남았다고 가정해보자.
  이 경우 남은 follower 역할을 수행하던 zookeeper server 중 랜덤한 zookeeper server 한대가 leader 역할을 수행하게 된다.
  이는 전체 앙상블을 이루던 4대 중 과반수인 3대가 정상적으로 서비스 되기 때문이다.


- 만약 추가적으로 1대의 zookeeper server가 문제가 발생해 2대의 zookeeper server만 남는 경우라면
  과반수(>=3)을 넘지 못함으로 문제가 없는 것으로 판단되었던 다른 2대의 zookeeper server도 서비스를 종료하게 된다.


- 3대로 zookeeper 앙상블을 구성한 경우도 동일하다. 과반수(>=2)를 유지하기 위해서는 1대의 결함만을 허용한다.
  4대의 구성과 3대의 구성으로 인한 결함 허용 수준이 1대로 다르지 않기 때문에 홀수로 zookeeper 앙상블을 구성하라고 권고하는 것이다.


- 결과적으로 결함 허용수준(F) 에 따라서 다음과 같이 앙상블을 구성하는 zookeeper server 수(N)을 결정하면 된다.

> N = 2*F+1

- 클라이언트 연결이 너무 많아서 5대 혹은 7대의 노드가 부하를 감당하기에 모자란다는 생각이 들면 옵저버 노드를 추가함으로써 읽기 전용 트래픽을 분산 시킬 수 있도록 한다.


- 옵저버는 리더 선출 시, 투표권이 없으며, 클라이언트가 보낸 요청의 적용 여부를 결정하는 과정에는 참여하지 않는다. 대신 옵저버는 리더와 팔로워가 결정한 변경사항을 학습한다. (readOnly & 복제만 한다..)


- 주키퍼 서버를 앙상블로 구성하려면?
1. 각 서버는 공통된 설정 파일을 사용해야 한다. 이 설정 파일에는 앙상블에 포함된 모든 서버의 목록이 포함되어 있다.


2. 각 서버는 데이터 디렉토리에 자신의 ID 번호를 지정하는 myid 파일을 가지고 있어야 한다

( 로컬에 주키퍼 설치하고 앙상블 구성하셨으면 아시겠져...)

```
tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=20
syncLimit=5
server.1=zoo1.example.com:2888:3888
server.2=zoo2.example.com:2888:3888
server.3=zoo3.example.com:2888:3888
```

- initLimit : 팔로워가 리더와 연결할 수 있는 초기화 제한 시간
- syncLimit : 팔로워가 리더와 연결할 수 있는 동기화 제한 시간
- 두 값 모두 tickTime 단위로 정의되는데, initLimit 값은 20 * 2000ms = 40s
- server.{X}={hostname}:{peerPort}:{leaderPort} 설정은 앙상블 내에 모든 서버 내역이다.
- X : 셔벼의 ID ( 정수값이기만 하면 됨. 0부터 시작할 필요도 없고 순차적으로 부여될 필요도 없음)
- hostname : 서버의 호스트명 또는 IP 주소
- peerPort : 앙상블 안의 서버들이 서로 통신할 때 사용하는 TCP 포트 번호
- leaderPort : 리더를 선출하는 데 사용되는 TCP 포트 번호
- 클라이언트는 clientPort에 지정된 포트번호로 앙상블에 연결만 할 수 있으면 된다. 하지만 앙상블의 멤버들은 세 포트를 모두 사용해서 서로 통신할 수 있어야 한다. ( 방화벽 포트 열렸는지 체크 필수~~~ )

### 2.2 카프카 브로커 설치하기

(kafka 다운받아서 압축 풀면 끝임..생략하겠음)

클러스터에 간단한 명령 몇 개를 실행 시켜서 제대로 동작하는지 확인할 수 있다.

- 토픽을 생성하고 확인한다.

```
# /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 \
-- create --replication-factor 1 --partitions 1 --topic test

# /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 \
-- describe --topic test
```

- test 토픽에 메시지를 쓴다. ( produce )

```
# /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server
localhost:9092 --topic test
> Test Message 1
> Test Message 2
```

- test 토픽에 메시지를 읽는다. (consume)

```
# /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server
localhost:9092 --topic test --from-beginnig
Test Message 1
Test Message 2
```

### 2.3 브로커 설정하기

( 이건 그냥 설정된 브로커 사용해서 해본 적은 없슴..)

- 특정한 활용 사례가 아닌 한, 바꿀 일이 없는 튜닝 관련 옵션들이기 때문에 기본값을 그냥 내버려둬도 상관없다..(ㅇㅇ...)

#### 2.3.1 핵심 브로커 매개변수

클러스터 모드로 작동 시키려면 해당 매개변수를 잘 알고 있어야 함~

단일 서버에서 단독으로 실행 되는 브로커는 걍 초기 설정대로 사용해도 상관없음..

1. broker.id

- 모든 카프카 브로커는 정숫값 식별자를 갖는다.
- 기본값은 0. 정숫값이면 어떠한 값도 가능하나 각 브로커별로 전부 달라야 한다.
- 유지보수를 위해 호스트 별로 고정된 값을 사용하는 것을 강력하게 권장한다.

2. listeners
- 구버전 카프카는 단순한 port 설정을 사용했으나 지원 중단되었다.
- 새로 도입된 listeners 설정은 쉼표로 구분된 리스너 이름과 URL 목록이다.
- 리스너 이름이 일반적인 보안 프로토콜(SSL 이런거 말하는 건가??)이 아니라면, 반드시 listener.security.protocol.map 설정을 잡아 주어야 한다.
- {프로토콜}://{호스트이름}:{포트} 형태로 정의된다.
- 예) PLAINTEXT://localhost:9092
- 호스트 이름을 0.0.0.0으로 잡아줄 경우 모든 네트워크 인터페이스로부터 연결을 받게 되며, 이 값을 비워 주면 기본 인터페이스에 대해서만 연결을 받게 된다.
- 1024 미만의 포트 번호를 사용할 경우 루트 권한으로 카프카를 실행시켜야 한다 -> 바람직하지 않음

3. zookeeper.connect
- 브로커의 메타데이터가 저장되는 주키퍼의 위치를 가리킨다.
- 세미콜론으로 연결된 {호스트이름}:{포트}/{경로}의 목록 형식으로 지정할 수 있다.
- /{경로} : 선택사항. 카프카 클러스트의 chroot 환경으로 사용될 주키퍼 경로. 지정하지 않으면 루트 디렉토리가 사용
- chroot 경로가 존재하지 않는 경로로 지정될 경우, 브로커가 시작될 때 자동으로 생성된다.
- chroot 지정해서 써라.. 다른 서비스 영향 안주고 받도록..

4. log.dirs
- 카프카는 모든 메시지를 로그 세그먼트 단위로 묵어서 log.dir 설정에 지정된 디스크 디렉토리에 저장한다.
- 다수의 디렉토리를 지정하고자 하는 경우, log.dirs를 사용한다.
- 브로커는 사용된 디스크 용량 기준이 아니라 저장된 파티션 기준으로 새 파티션을 저장한다 -> 균등한 양의 데이터가 저장되지 않는 구조 -> 참고해라!!

5. num.recovery.threads.per.data.dir
- 카프카는 설정 가능한 스레드 풀을 사용해서 로그 세그먼트를 관리한다.
- 스레드 풀이 뭘 하는데?
1. 브로커가 정상적으로 시작되었을 때, 각 파티션의 로그 세그먼트 파일을 연다.
2. 브로커가 장애 발생 후 다시 시작되었을 때, 각 파티션의 로그 세그먼트를 검사하고 잘못된 부분은 삭제한다.
3. 브로커가 종료할 때, 로그 세그먼트를 정상적으로 닫는다.
- 기본적으로 하나의 로그 디렉토리에 하나의 스레드만이 사용된다.
- 이 스레드들은 브로커가 시작 / 종료될 때만 사용하기 때문에 작업을 병렬화하기 위해서는 많은 수의 스레드를 할당해주는 게 좋다.
- ( 빠른 시작 / 복구를 위해..~)
- 이 설정값은 log.dirs 에 지정된 로그 디렉토리별 스레드 수 ->  num.recovery.threads.per.data.dir(8) * log.dirs(3) = 전체 스레드 수 (24)

6. auto.create.topics.enable

- 카프카 기본 설정에는 아래와 같은 상황에서 브로커가 토픽을 자동으로 생성하도록 되어있다.
1. 프로듀서가 토픽에 메시지를 쓰기 시작할 때
2. 컨슈머가 토픽으로부터 메시지를 읽기 시작할 때
3. 클라이언트가 토픽에 대한 메타데이터를 요청할 때

- 이게 맞나 싶으면 false 로 설정해라.. (실무에서는 false 해야할듯..? topic 관리가 안될 듯..)

7. auto.leader.rebalance.enable

- 이 설정을 활성화해주면 리더 역할이 균등하게 분산하여 하나의 브로커에 리더 역할이 집중되는 현상을 방지한다.
- 이 설정을 켜면 파티션의 분포 상태를 주기적으로 확인하는 백그라운드 스레드가 시작된다.
- 전체 파티션 중 특정 브로커에 리더 역할이 할당된 파티션 비율이 leader.imbalance.per.broker.percentage 에 설정된 값을 넘어가면 파티션의 선호 리더 리밸런싱이 발생한다.

8. delete.topic.enable

- 클러스터의 토픽을 임의로 삭제하지 못하게끔 막아야 할 때, false 로 설정한다.
- ( 그럼 삭제할려면 true 로 변경해야겠네..? )

#### 2.3.2 토픽별 기본값

1. num.partitions
- 새로운 토픽이 생성될 때, 몇 개의 파티션을 갖게 되는지를 결정하며 주로 자동 토픽 생성 기능이 활성화되어 있을 때 사용된다.
- 기본값은 1
- 토픽의 파티션 개수는 늘릴 수만 있지 줄일 수는 없으니 점진적으로 늘려라..?
- 많은 사용자들은 토픽당 파티션 개수를 클러스터 내 브로커 수와 맞추거나 아니면 배수로 설정한다.
- (우리는 개발은 3 / 상용은 5개로 하는듯..? 이유가 뭘까? 생각해본 사람~~?)

2. default.replication.factor
- 자동 토픽 생성 기능이 활성화되어 있을 경우, 이 설정은 새로 생성되는 토픽의 복제 팩터를 결정한다.
- 토픽 단위로 replication factor 를 설정한다 해서 토픽을 replication 하는 것이 아니라, 토픽을 이루는 각각의 파티션을 replication 한다.
- (이 부분은 7장에서 더 자세히 보는걸로)

3. log.retention.ms
- 카프카가 얼마나 오랫동안 메시지를 보존해야 하는지를 지정할 때 가장 많이 사용되는 설정이 시간 기준 보존 주기 설정이다.
- 설정 파일에 정의되어 있는 기본값은 log.retention.hours 설정을 사용하며, 168시간(=1주일) 이다.
- minutes / ms 단위도 사용할 수 있으며, 우선순위는 더 작은 단위 설정값이 높다 ( = ms)

4. log.retention.bytes
- 메시지 만료의 또 다른 기준은 보존되는 메시지의 용량이다.
- 파티션 단위로 적용된다. -> 파티션 개수 (8) * log.retention.bytes (1GB) = 8GB
- -1 로 설정하면 데이터 영구 보존

5. log.segment.bytes
- 로그 세그먼트의 크기가 log.segment.bytes 에 지정된 크기에 다다르면 브로커는 기존 로그 세그먼트를 닫고 새로운 세그먼트를 연다.
- 로그 세그먼트는 닫히기 전까지는 만료와 삭제의 대상이 되지 않는다.

6. log.roll.ms
- 로그 세그먼트 파일이 닫히는 시점 제어를 시간 단위로 지정한다.
- 카프카는 크기 제한이든 시간 제한이든 하나라도 도달하는 경우 로그 세그먼트를 닫는다.
- 기본적으로 log.segment.bytes 설정만 되어있고 log.roll.ms 는 설정되어 있지 않다.

7. min.insync.replicas
- 최신 상태로 프로듀서와 동기화 되어야 하는 레플리카 수를 설정한다.
- 2로 잡아주면 프로듀서의 쓰기 작업이 성공하기 위해 최소 2개의 레플리카가 응답해야한다.

8. message.max.bytes
- 카프카 브로커가 쓸 수 있는 메시지의 최대 크기
- 기본값은 1MB
- message.max.bytes 값 보다 더 큰 메시지를 보내면 에러가 발생한다.
- 이 설정은 압축된 메시지 크기를 기준으로 한다.
- message.max.bytes 는 컨슈머 클라이언트의 fetch.message.max.bytes, 클러스터 브로커의 replica.fetch.max.bytes 설정과 맞아야 한다.


### 2.4 하드웨어 선택하기

디스크 처리량과 용량, 메모리, 네트워크, CPU를 감안해야한다.

#### 2.4.1 디스크 처리량
- 로그 세그먼트를 저장하는 브로커 디스크의 처리량은 프로듀서 클라이언트 성능에 가장 큰 영향을 미친다.
- 많은 수의 클라이언트 연결을 받아내야 하는 경우 SSD
- 자주 쓸 일이 없는 데이터를 굉장히 많이 저장해야 하는 클러스터의 경우 HDD

#### 2.4.2 디스크 용량
- (메시지 저장해야하니깐..중요하겠지..)
- 필요한 디스크 용량은 클러스터에 설정된 복제방식에 따라서도 달라질 수 있다.

#### 2.4.3 메모리
- 카프카 컨슈머는 프로듀서가 막 추가한 메시지를 바로 뒤에서 쫒아가는 식으로 파티션의 맨 끝에서 메시지를 읽어 오는 것이 보통이다.
- 최적의 작동은 시스템의 페이지 캐시에 저장된 메시지들을 컨슈머가 읽어 오는 것 -> 페이지 캐시로 메모리를 더 할당해주면 컨슈머 클라이언트의 성능이 향상
- 카프카를 하나의 시스템에서 다른 애플리케이션과 함께 운영하는 것을 권장하지 않는 주된 이유가 페이지 캐시를 나눠쓰지 않도록 하기 위해서다. -> 카프카의 컨슈머 성능 저하

#### 2.4.4 네트워크
- 사용 가능한 네트워크 대역폭은 카프카가 처리할 수 있는 트래픽 최대량을 결정한다.
- 디스크 용량과 함께 클러스터 크기를 결정하는 가장 결정적인 요인이다.

#### 2.4.5 CPU
- 디스크나 메모리만큼 중요하지 않으나 메시지 압축 / 해제 시 영향을 주는 부분이다.

### 2.5 클라우드에서 카프카 사용하기

마이크로소프트 애저, 아마존 웹 서비스, 구글 클라우드 플랫폼과 같은 클라우드 컴퓨팅 환경에서 카프카를 많이 설치한다.

### 2.6 카프카 클러스터 설정하기

상용 환경에서는 여러대의 브로커를 하나의 클러스터로 구성해야한다.

- 가장 큰 이점은 부하를 다수의 서버로 확장할 수 있다.
- 그 다음 이점은 복제를 사용함으로써 데이터 유실을 방지할 수 있다.

#### 2.6.1 브로커 개수

- 디스크 용량
- 브로커당 레프리카 용량
- CPU 용량
- 네트워크 용량

#### 2.6.2 브로커 설정

- 다수의 카프카 브로커가 하나의 클러스터를 이루게 하기 위해서 설정해 줘야 하는 건 두 개
1. 모든 브로커들이 동일한 zookeeper.connect 설정값을 가져야한다.
2. 클러스터 안의 모든 브로커가 유일한 broker.id 설정값을 가져야한다.

#### 2.6.3 운영체제 튜닝하기

- 리눅스 배포판은 웬만한 애플리케이션에 대해 잘 작동하도록 커널 튜닝 매개변수가 미리 잡혀있으나, 몇 가지를 변경하면 카프카 브로커의 성능을 끌어올릴 수 있다.
- 주로 가상 메모리와 네트워크 서브시스템, 로그 세그먼트를 저장하기 위해 사용되는 디스크의 마운트 등이다.
- 이 매개변수들은 대게 /etc/sysctl.conf 에 설정된다.

(이건 관심있음 각자 더 알아보는걸로..)

### 2.7 프로덕션 환경에서의 고려사항

#### 2.7.1 가비지 수집기 옵션
- MaxGCPauseMills : 각 가비지 수집 사이클에 있어서 선호되는 중단 시간을 지정..가비지 수집기가 호출되는 주기? 단위 사이클에 걸리는 시간?
- InitiatingHeapOccupancyPercent : G1GC 수집 사이클을 시작하기 전까지 전체 힙에서 사용 가능한 비율을 백분율로 지정한 값
- 기본값 45. 전체 힙의 45%가 사용되기 전까지는 G1GC가 가비지 수집 사이클을 시작하지 않는다는 의미이다.

#### 2.7.2 데이터센터 레이아웃
- 카프카 브로커의 물리적 위치가 중요한 이유는 중단되는 일이 발생할 때, 서비스 제공이 불가능하거나 사용자 활동을 추척할 수 없기 때문이다.
- 장애 영역 개념이 있는 데이터 센터 환경이 좋으며, 브로커가 데이터 센터의 랙에서 차지하는 물리적인 위치 / 복제가 중요한 이유다.
- 서버 랙이란? 데이터 센터 또는 서버실에서 표준화된 방식으로 여러 개의 서버를 저장하고 정리할 수 있는 특수 캐비닛 또는 프레임
- 카프카 클러스터의 각 브로커가 서로 다른 랙에 설치되도록 하거나 아니면 단일 장애점이 없도록 하는게 모범담안이다.

#### 2.7.3 주키퍼 공유하기
- 주키퍼는 카프카의 브로커, 토픽, 파티션에 대한 메타데이터 정보를 저장하기 위해 사용한다.
- 카프카 클러스터가 하나의 주키퍼 앙상블을 공유하는 경우를 제외하면, 다른 어플리케이션과 주키퍼 앙상블을 공유하는 것은 피하자.




